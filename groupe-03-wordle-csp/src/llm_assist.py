"""
Module d'assistance LLM (Large Language Model) - STUB P√âDAGOGIQUE

Ce module est un STUB (d√©sactiv√© par d√©faut) qui d√©montre COMMENT
un LLM pourrait √™tre int√©gr√© au solveur Wordle CSP.

IMPORTANT:
- Ce module ne n√©cessite AUCUNE cl√© API
- Il s'agit d'une d√©monstration p√©dagogique
- Les fonctions retournent des r√©ponses simul√©es

PERSPECTIVES D'INT√âGRATION LLM:

1. Explication du raisonnement:
   - Le LLM pourrait expliquer pourquoi un mot est sugg√©r√©
   - Vulgariser les concepts CSP pour l'utilisateur
   - Fournir du contexte linguistique

2. Proposition de mots:
   - Le LLM pourrait proposer des mots bas√©s sur:
     * S√©mantique (mots li√©s au contexte)
     * Phon√©tique (mots qui sonnent similaires)
     * Fr√©quence dans la langue

3. Hybridation neuro-symbolique:
   - IA symbolique (CSP): garantit la coh√©rence logique
   - IA neuronale (LLM): apporte l'intuition et le contexte
   - Combinaison: meilleure performance + explicabilit√©

LIMITATIONS:
- Les LLM ne garantissent pas la coh√©rence logique
- Le CSP reste n√©cessaire pour le filtrage rigoureux
- Le LLM est un compl√©ment, pas un remplacement
"""

from typing import List, Dict
import random


# Configuration (d√©sactiv√© par d√©faut)
LLM_ENABLED = False


def is_llm_available() -> bool:
    """
    V√©rifie si un LLM est disponible.

    Returns:
        False (stub p√©dagogique)
    """
    return LLM_ENABLED


def explain_suggestion(word: str, candidates: List[str], constraints: List) -> str:
    """
    G√©n√®re une explication humaine pour une suggestion de mot.

    Dans une vraie impl√©mentation, le LLM recevrait:
    - Le mot sugg√©r√©
    - Les contraintes actuelles
    - L'historique des tentatives

    Et retournerait une explication en langage naturel.

    Args:
        word: mot sugg√©r√©
        candidates: liste des candidats restants
        constraints: contraintes CSP appliqu√©es

    Returns:
        Explication textuelle (simul√©e)

    Exemple de sortie:
        "Je sugg√®re GERER car:
         - Il contient 'E' qui appara√Æt dans 80% des candidats
         - Il utilise la lettre 'R' en position 2, ce qui √©limine 40% des mots restants
         - C'est un mot fr√©quent en fran√ßais, augmentant les chances"
    """
    if not LLM_ENABLED:
        return _simulate_explanation(word, candidates)

    # Dans une vraie impl√©mentation:
    # prompt = f"Explique pourquoi sugg√©rer '{word}' sachant que..."
    # response = llm_api.complete(prompt)
    # return response

    return _simulate_explanation(word, candidates)


def _simulate_explanation(word: str, candidates: List[str]) -> str:
    """Simule une explication LLM."""
    reasons = [
        f"Le mot '{word}' contient des lettres fr√©quentes dans les {len(candidates)} candidats restants",
        f"Ce choix maximise l'entropie, permettant d'√©liminer efficacement les possibilit√©s",
        f"'{word}' est un mot courant en fran√ßais, augmentant la probabilit√© de succ√®s"
    ]

    explanation = f"üí° Je sugg√®re {word} car:\n"
    for reason in random.sample(reasons, 2):
        explanation += f"  - {reason}\n"

    return explanation


def suggest_word_with_context(candidates: List[str], context: str = None) -> Dict:
    """
    Sugg√®re un mot en tenant compte du contexte linguistique.

    Un LLM pourrait proposer des mots bas√©s sur:
    - Le contexte th√©matique ("mots li√©s √† la nature")
    - La structure phon√©tique
    - Les associations s√©mantiques

    Args:
        candidates: liste des candidats CSP
        context: contexte optionnel (ex: "animaux", "nature")

    Returns:
        Dictionnaire avec suggestion et explication
    """
    if not candidates:
        return {
            'word': None,
            'explanation': "Aucun candidat disponible"
        }

    # Dans une vraie impl√©mentation:
    # Le LLM recevrait le contexte et proposerait un mot
    # Puis le CSP validerait que ce mot est dans les candidats

    word = random.choice(candidates)
    explanation = _simulate_explanation(word, candidates)

    return {
        'word': word,
        'explanation': explanation,
        'context_used': context is not None
    }


def explain_csp_concept(concept: str) -> str:
    """
    Explique un concept CSP en langage naturel.

    Utile pour la vulgarisation p√©dagogique.

    Args:
        concept: nom du concept ('variable', 'domaine', 'contrainte', 'arc-consistency')

    Returns:
        Explication en fran√ßais

    Exemple:
        >>> explain_csp_concept('arc-consistency')
        "L'arc-consistency (coh√©rence d'arc) est une technique de propagation
        de contraintes qui √©limine les valeurs incompatibles dans les domaines..."
    """
    explanations = {
        'variable': """
        Une VARIABLE dans un CSP repr√©sente un √©l√©ment √† d√©terminer.
        Dans Wordle, on peut mod√©liser chaque position (1-5) comme une variable,
        ou consid√©rer une seule variable 'mot' dont le domaine est le dictionnaire.
        """,

        'domaine': """
        Le DOMAINE d'une variable est l'ensemble des valeurs possibles.
        Dans Wordle, le domaine initial est l'ensemble de tous les mots de 5 lettres.
        Apr√®s chaque feedback, le domaine se r√©duit.
        """,

        'contrainte': """
        Une CONTRAINTE limite les valeurs possibles des variables.
        Dans Wordle, les contraintes sont:
        - Lettres vertes: position exacte
        - Lettres jaunes: pr√©sence mais mauvaise position
        - Lettres grises: absence
        """,

        'arc-consistency': """
        L'ARC-CONSISTENCY (coh√©rence d'arc) est une technique de propagation.
        Elle √©limine les valeurs du domaine qui ne peuvent satisfaire aucune solution.
        Dans Wordle, on filtre tous les mots incompatibles avec les feedbacks re√ßus.
        """,

        'heuristique': """
        Une HEURISTIQUE est une r√®gle intuitive pour guider la recherche.
        Dans Wordle, nos heuristiques choisissent le mot qui:
        - Maximise les lettres fr√©quentes (heuristique fr√©quence)
        - Maximise l'information gagn√©e (heuristique entropie)
        """
    }

    return explanations.get(
        concept,
        f"Concept '{concept}' non document√© dans ce stub."
    )


def hybrid_neuro_symbolic_suggestion(
    symbolic_suggestion: str,
    candidates: List[str],
    use_llm_boost: bool = False
) -> Dict:
    """
    D√©montre une approche hybride neuro-symbolique.

    Principe:
    1. Le syst√®me symbolique (CSP) filtre les candidats valides
    2. Le syst√®me neuronal (LLM) ordonne les candidats par pertinence
    3. La combinaison donne le meilleur des deux mondes

    Args:
        symbolic_suggestion: suggestion du CSP
        candidates: candidats valides selon le CSP
        use_llm_boost: utiliser le LLM pour r√©-ordonner

    Returns:
        Suggestion finale avec explication
    """
    result = {
        'symbolic_suggestion': symbolic_suggestion,
        'final_suggestion': symbolic_suggestion,
        'llm_used': False,
        'explanation': "Suggestion purement symbolique (CSP)"
    }

    if use_llm_boost and LLM_ENABLED:
        # Dans une vraie impl√©mentation:
        # 1. Demander au LLM de scorer chaque candidat
        # 2. Combiner avec le score CSP
        # 3. Retourner le meilleur

        result['llm_used'] = True
        result['explanation'] = "Suggestion hybride: CSP + LLM"

    return result


def generate_educational_summary(solver_stats: Dict) -> str:
    """
    G√©n√®re un r√©sum√© p√©dagogique de la r√©solution.

    Utile pour expliquer le processus √† un √©tudiant.

    Args:
        solver_stats: statistiques du solveur CSP

    Returns:
        R√©sum√© en fran√ßais
    """
    summary = f"""
    üìö R√âSUM√â P√âDAGOGIQUE

    √âtat initial:
    - {solver_stats.get('initial_candidates', 0)} mots dans le dictionnaire

    Apr√®s r√©solution:
    - {solver_stats.get('constraints_applied', 0)} contraintes appliqu√©es
    - {solver_stats.get('current_candidates', 0)} candidats restants
    - Taux de r√©duction: {solver_stats.get('reduction_rate', 0):.1%}

    Principe CSP:
    Chaque feedback ajoute une contrainte qui √©limine les mots incompatibles.
    C'est une approche symbolique: logique pure, pas de "devinette".

    L'IA exploratoire optimise le choix des mots pour minimiser le nombre
    de coups n√©cessaires.
    """

    return summary.strip()


# Exemple d'utilisation p√©dagogique
if __name__ == '__main__':
    print("Module LLM Assist - Stub p√©dagogique")
    print("=" * 60)
    print(f"LLM activ√©: {is_llm_available()}")
    print()

    # D√©monstration des explications
    print("Explication du concept 'contrainte':")
    print(explain_csp_concept('contrainte'))
    print()

    # Simulation de suggestion
    candidates = ['ARBRE', 'AUTRE', 'AITRE']
    result = suggest_word_with_context(candidates)
    print(f"Suggestion simul√©e: {result['word']}")
    print(result['explanation'])
